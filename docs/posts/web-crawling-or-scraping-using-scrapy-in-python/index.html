<!doctype html><html lang=en><head><title>web crawling or scraping using scrapy in python :: Varun Pant — WebLog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
To Run , simply type scrapy runspider scraper.py
Running, above code will output something like below
2018-12-02 14:01:18 [scrapy.utils.log] INFO: Scrapy 1."><meta name=keywords content="Cloud,maps,GIS,Google Cloud,Google Maps,Openlayer,Javascript,go,Hadoop,BigData,Spark"><meta name=robots content="noodp"><link rel=canonical href=https://varunpant.com/posts/web-crawling-or-scraping-using-scrapy-in-python/><link type=application/opensearchdescription+xml rel=search title="Varun Pant" href=//www.varunpant.com/opensearch.axd><link rel=me type=text/html href="//plus.google.com/108455320594011020517?rel=me"><link rel=me type=text/html href=//varunpant.com/feed><link rel=me type=text/html href=//twitter.com/varunpant><link rel=me type=text/html href=//www.facebook.com/varun.pant><meta name=msvalidate.01 content="B96ED99B0213B547253C91D27272A5E4"><meta name=google-site-verification content="yu9tlev9-FkUUc0UkPrKP-8ren886--1FcSINU0TJDY"><meta http-equiv=x-xrds-location content="//www.myopenid.com/xrds?username=varunpant.myopenid.com"><meta name=syndication content="//feeds.feedburner.com/varun.pant"><link rel=stylesheet href=https://varunpant.com/assets/style.css><link rel=stylesheet href=https://varunpant.com/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://varunpant.com/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://varunpant.com/img/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:title content="web crawling or scraping using scrapy in python"><meta name=twitter:description content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
To Run , simply type scrapy runspider scraper.py
Running, above code will output something like below
2018-12-02 14:01:18 [scrapy.utils.log] INFO: Scrapy 1."><meta property="og:title" content="web crawling or scraping using scrapy in python"><meta property="og:description" content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
To Run , simply type scrapy runspider scraper.py
Running, above code will output something like below
2018-12-02 14:01:18 [scrapy.utils.log] INFO: Scrapy 1."><meta property="og:type" content="article"><meta property="og:url" content="https://varunpant.com/posts/web-crawling-or-scraping-using-scrapy-in-python/"><meta property="article:published_time" content="2018-12-02T00:00:00+00:00"><meta property="article:modified_time" content="2018-12-02T00:00:00+00:00"><meta property="og:site_name" content="Varun Pant"><section><div id=ta class=advert><script type=text/javascript>google_ad_client="ca-pub-1110730152886910";google_ad_slot="7384719372";google_ad_width=468;google_ad_height=60;</script><script type=text/javascript src=//pagead2.googlesyndication.com/pagead/show_ads.js></script></div></section></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h2 class=post-title><a href=https://varunpant.com/posts/web-crawling-or-scraping-using-scrapy-in-python/>web crawling or scraping using scrapy in python</a></h2><div class=post-meta><span class=post-date>2018-12-02</span>
<span class=post-author>— Varun Pant</span>
<span class=post-read-time>— 3 min read</span></div><span class=post-tags>#<a href=https://varunpant.com/tags/python/>python</a>&nbsp;</span><div class=post-content><p>Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.</p><p>In this post, I will demonstrate creating a very basic web crawler.</p><h3 id=install-scrapy>Install Scrapy</h3><p>Installation is via pip pip install scrapy</p><h3 id=minimalistic-code>Minimalistic Code</h3><p>A very simple scraper is created like this</p><pre><code class=language-import data-lang=import></code></pre><p>To <strong>Run</strong> , simply type scrapy runspider scraper.py</p><p>Running, above code will output something like below</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.utils.log<span style=color:#f92672>]</span> INFO: Scrapy 1.5.1 started <span style=color:#f92672>(</span>bot: scrapybot<span style=color:#f92672>)</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.utils.log<span style=color:#f92672>]</span> INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 2.7.15 <span style=color:#f92672>(</span>default, Oct <span style=color:#ae81ff>2</span> 2018, 11:42:04<span style=color:#f92672>)</span> - <span style=color:#f92672>[</span>GCC 4.2.1 Compatible Apple LLVM 10.0.0 <span style=color:#f92672>(</span>clang-1000.11.45.2<span style=color:#f92672>)</span><span style=color:#f92672>]</span>, pyOpenSSL 18.0.0 <span style=color:#f92672>(</span>OpenSSL 1.1.0j <span style=color:#ae81ff>20</span> Nov 2018<span style=color:#f92672>)</span>, cryptography 2.4.2, Platform Darwin-18.2.0-x86<span style=color:#ae81ff>\_</span>64-i386-64bit 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.crawler<span style=color:#f92672>]</span> INFO: Overridden settings: <span style=color:#f92672>{</span><span style=color:#e6db74>&#39;SPIDER\_LOADER\_WARN\_ONLY&#39;</span>: True<span style=color:#f92672>}</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.middleware<span style=color:#f92672>]</span> INFO: Enabled extensions: <span style=color:#f92672>[</span><span style=color:#e6db74>&#39;scrapy.extensions.memusage.MemoryUsage&#39;</span>, <span style=color:#e6db74>&#39;scrapy.extensions.logstats.LogStats&#39;</span>, <span style=color:#e6db74>&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span>, <span style=color:#e6db74>&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span style=color:#f92672>]</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.middleware<span style=color:#f92672>]</span> INFO: Enabled downloader middlewares: <span style=color:#f92672>[</span><span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span style=color:#f92672>]</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.middleware<span style=color:#f92672>]</span> INFO: Enabled spider middlewares: <span style=color:#f92672>[</span><span style=color:#e6db74>&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span>, <span style=color:#e6db74>&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span style=color:#f92672>]</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.middleware<span style=color:#f92672>]</span> INFO: Enabled item pipelines: <span style=color:#f92672>[</span><span style=color:#f92672>]</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.core.engine<span style=color:#f92672>]</span> INFO: Spider opened 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.extensions.logstats<span style=color:#f92672>]</span> INFO: Crawled <span style=color:#ae81ff>0</span> pages <span style=color:#f92672>(</span>at <span style=color:#ae81ff>0</span> pages/min<span style=color:#f92672>)</span>, scraped <span style=color:#ae81ff>0</span> items <span style=color:#f92672>(</span>at <span style=color:#ae81ff>0</span> items/min<span style=color:#f92672>)</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.extensions.telnet<span style=color:#f92672>]</span> DEBUG: Telnet console listening on 127.0.0.1:6023 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.core.engine<span style=color:#f92672>]</span> DEBUG: Crawled <span style=color:#f92672>(</span>200<span style=color:#f92672>)</span> &lt;get https:<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span> varunpant.com<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>&gt; <span style=color:#f92672>(</span>referer: None<span style=color:#f92672>)</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.core.engine<span style=color:#f92672>]</span> INFO: Closing spider <span style=color:#f92672>(</span>finished<span style=color:#f92672>)</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.statscollectors<span style=color:#f92672>]</span> INFO: Dumping Scrapy stats: <span style=color:#f92672>{</span><span style=color:#e6db74>&#39;downloader/request\_bytes&#39;</span>: 212, <span style=color:#e6db74>&#39;downloader/request\_count&#39;</span>: 1, <span style=color:#e6db74>&#39;downloader/request\_method\_count/GET&#39;</span>: 1, <span style=color:#e6db74>&#39;downloader/response\_bytes&#39;</span>: 12403, <span style=color:#e6db74>&#39;downloader/response\_count&#39;</span>: 1, <span style=color:#e6db74>&#39;downloader/response\_status\_count/200&#39;</span>: 1, <span style=color:#e6db74>&#39;finish\_reason&#39;</span>: <span style=color:#e6db74>&#39;finished&#39;</span>, <span style=color:#e6db74>&#39;finish\_time&#39;</span>: datetime.datetime<span style=color:#f92672>(</span>2018, 12, 2, 14, 1, 18, 850998<span style=color:#f92672>)</span>, <span style=color:#e6db74>&#39;log\_count/DEBUG&#39;</span>: 2, <span style=color:#e6db74>&#39;log\_count/INFO&#39;</span>: 7, <span style=color:#e6db74>&#39;memusage/max&#39;</span>: 50581504, <span style=color:#e6db74>&#39;memusage/startup&#39;</span>: 50577408, <span style=color:#e6db74>&#39;response\_received\_count&#39;</span>: 1, <span style=color:#e6db74>&#39;scheduler/dequeued&#39;</span>: 1, <span style=color:#e6db74>&#39;scheduler/dequeued/memory&#39;</span>: 1, <span style=color:#e6db74>&#39;scheduler/enqueued&#39;</span>: 1, <span style=color:#e6db74>&#39;scheduler/enqueued/memory&#39;</span>: 1, <span style=color:#e6db74>&#39;start\_time&#39;</span>: datetime.datetime<span style=color:#f92672>(</span>2018, 12, 2, 14, 1, 18, 570736<span style=color:#f92672>)</span><span style=color:#f92672>}</span> 2018-12-02 14:01:18 <span style=color:#f92672>[</span>scrapy.core.engine<span style=color:#f92672>]</span> INFO: Spider closed <span style=color:#f92672>(</span>finished<span style=color:#f92672>)</span> Varun Pant Blog | Index
</code></pre></div><p>In the last line, one can see that the page title <strong>Varun Pant Blog | Index</strong> was printed.</p><p>The principle of a web scraper is generally quite simple,</p><ol start=2><li>Start from a seed url and extract all hyperlinks.</li><li>Then crawl each of them, further extracting links until all of them have been visited.</li></ol><p>In Scrapy start_urls is the list of seed urls to begin scraping.</p><p>One can also provide another varibale to add domain constraint. allowed_domains = [&ldquo;varunpant.com&rdquo;] so that links which lead out of the principal domain are rejected.</p><p>Scrappy takes care of ensuring that links which have already been visited are not visited again.</p><h3 id=extracting-links>Extracting Links</h3><p>In the simple example above, the code does not extract hyperlinks from the response body, lets modify it to do so.</p><pre><code>import scrapy class MySpider(scrapy.Spider): name = &quot;MySpider&quot; start\_urls = ['https://varunpant.com'] def parse(self, response): all\_links = response.xpath('*//a/@href').extract() for link in all\_links: print(link)
</code></pre><p>Line all_links = response.xpath('*//a/@href&rsquo;).extract() , uses xpath and extracts all hyperlinks in the page.</p><pre><code class=language-/ data-lang=/></code></pre><p>The final step is to make the parse generator method yield response like so</p><pre><code class=language-import data-lang=import></code></pre><p>This should crawl all hyperlinks found with each page.</p><h3 id=save-all-crawlled-pages-to-link>Save all crawlled pages to link</h3><p>Here is a minimalistic code snippet, which saves all crawlled pages to the disk.</p><pre><code class=language-import data-lang=import></code></pre></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://varunpant.com/posts/download-file-using-webdriver-firefox-and-python-selenium/><span class=button__icon>←</span>
<span class=button__text>download file using webdriver firefox and python selenium</span></a></span>
<span class="button next"><a href=https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/><span class=button__text>integration testing with apache beam using pubsub and bigtable emulators and direct runner</span>
<span class=button__icon>→</span></a></span></div></div><div id=blog-comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"varunpant"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2019 Powered by <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by <a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://varunpant.com/assets/main.js></script><script src=https://varunpant.com/assets/prism.js></script><section><div class=advert id=ta2><script type=text/javascript>google_ad_client="ca-pub-1110730152886910";google_ad_slot="1639236970";google_ad_width=728;google_ad_height=90;</script><script type=text/javascript src=//pagead2.googlesyndication.com/pagead/show_ads.js></script></div></section><script type=text/javascript>(function(){window.disqus_shortname='varunpant';var s=document.createElement('script');s.async=true;s.type='text/javascript';s.src='https://'+disqus_shortname+'.disqus.com/count.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(s);}());</script></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-16978408-1','auto');ga('send','pageview');}</script></body></html>
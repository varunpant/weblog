<!doctype html><html lang=en><head><title>web crawling or scraping using scrapy in python :: Varun Pant — WebLog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
import scrapy class MySpider(scrapy.Spider): name = &amp;ldquo;MySpider&amp;rdquo; start_urls = [&amp;lsquo;https://varunpant.com'] def parse(self, response): # Get page title using xpath. page_title = response."><meta name=keywords content="Cloud,maps,GIS,Google Cloud,Google Maps,Openlayer,Javascript,go,Hadoop,BigData,Spark"><meta name=robots content="noodp"><link rel=canonical href=/posts/web-crawling-or-scraping-using-scrapy-in-python/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon.png><meta name=twitter:card content="summary"><meta name=twitter:title content="web crawling or scraping using scrapy in python"><meta name=twitter:description content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
import scrapy class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; start_urls = [&lsquo;https://varunpant.com'] def parse(self, response): # Get page title using xpath. page_title = response."><meta property="og:title" content="web crawling or scraping using scrapy in python"><meta property="og:description" content="Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.
In this post, I will demonstrate creating a very basic web crawler.
Install Scrapy Installation is via pip pip install scrapy
Minimalistic Code A very simple scraper is created like this
import scrapy class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; start_urls = [&lsquo;https://varunpant.com'] def parse(self, response): # Get page title using xpath. page_title = response."><meta property="og:type" content="article"><meta property="og:url" content="/posts/web-crawling-or-scraping-using-scrapy-in-python/"><meta property="article:published_time" content="2018-12-02T00:00:00+00:00"><meta property="article:modified_time" content="2018-12-02T00:00:00+00:00"><meta property="og:site_name" content="Varun Pant"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h2 class=post-title><a href=/posts/web-crawling-or-scraping-using-scrapy-in-python/>web crawling or scraping using scrapy in python</a></h2><div class=post-meta><span class=post-date>2018-12-02</span>
<span class=post-author>— Varun Pant</span>
<span class=post-read-time>— 4 min read</span></div><div class=post-content><p>Scrapy is a very popular web scraping/crawling framework, I have been using it for quite some time now.</p><p>In this post, I will demonstrate creating a very basic web crawler.</p><h3 id=install-scrapy>Install Scrapy</h3><p>Installation is via pip pip install scrapy</p><h3 id=minimalistic-code>Minimalistic Code</h3><p>A very simple scraper is created like this</p><p>import scrapy class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; start_urls = [&lsquo;<a href="https://varunpant.com'">https://varunpant.com'</a>] def parse(self, response): # Get page title using xpath. page_title = response.xpath('//title/text()').extract_first() print(page_title) To <strong>Run</strong> , simply type scrapy runspider scraper.py</p><p>Running, above code will output something like below</p><p>2018-12-02 14:01:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot) 2018-12-02 14:01:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 2.7.15 (default, Oct 2 2018, 11:42:04) - [GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.2)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0j 20 Nov 2018), cryptography 2.4.2, Platform Darwin-18.2.0-x86_64-i386-64bit 2018-12-02 14:01:18 [scrapy.crawler] INFO: Overridden settings: {&lsquo;SPIDER_LOADER_WARN_ONLY&rsquo;: True} 2018-12-02 14:01:18 [scrapy.middleware] INFO: Enabled extensions: [&lsquo;scrapy.extensions.memusage.MemoryUsage&rsquo;, &lsquo;scrapy.extensions.logstats.LogStats&rsquo;, &lsquo;scrapy.extensions.telnet.TelnetConsole&rsquo;, &lsquo;scrapy.extensions.corestats.CoreStats&rsquo;] 2018-12-02 14:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares: [&lsquo;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.retry.RetryMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&rsquo;, &lsquo;scrapy.downloadermiddlewares.stats.DownloaderStats&rsquo;] 2018-12-02 14:01:18 [scrapy.middleware] INFO: Enabled spider middlewares: [&lsquo;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&rsquo;, &lsquo;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&rsquo;, &lsquo;scrapy.spidermiddlewares.referer.RefererMiddleware&rsquo;, &lsquo;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&rsquo;, &lsquo;scrapy.spidermiddlewares.depth.DepthMiddleware&rsquo;] 2018-12-02 14:01:18 [scrapy.middleware] INFO: Enabled item pipelines: [] 2018-12-02 14:01:18 [scrapy.core.engine] INFO: Spider opened 2018-12-02 14:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2018-12-02 14:01:18 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023 2018-12-02 14:01:18 [scrapy.core.engine] DEBUG: Crawled (200) (referer: None) 2018-12-02 14:01:18 [scrapy.core.engine] INFO: Closing spider (finished) 2018-12-02 14:01:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {&lsquo;downloader/request_bytes&rsquo;: 212, &lsquo;downloader/request_count&rsquo;: 1, &lsquo;downloader/request_method_count/GET&rsquo;: 1, &lsquo;downloader/response_bytes&rsquo;: 12403, &lsquo;downloader/response_count&rsquo;: 1, &lsquo;downloader/response_status_count/200&rsquo;: 1, &lsquo;finish_reason&rsquo;: &lsquo;finished&rsquo;, &lsquo;finish_time&rsquo;: datetime.datetime(2018, 12, 2, 14, 1, 18, 850998), &lsquo;log_count/DEBUG&rsquo;: 2, &lsquo;log_count/INFO&rsquo;: 7, &lsquo;memusage/max&rsquo;: 50581504, &lsquo;memusage/startup&rsquo;: 50577408, &lsquo;response_received_count&rsquo;: 1, &lsquo;scheduler/dequeued&rsquo;: 1, &lsquo;scheduler/dequeued/memory&rsquo;: 1, &lsquo;scheduler/enqueued&rsquo;: 1, &lsquo;scheduler/enqueued/memory&rsquo;: 1, &lsquo;start_time&rsquo;: datetime.datetime(2018, 12, 2, 14, 1, 18, 570736)} 2018-12-02 14:01:18 [scrapy.core.engine] INFO: Spider closed (finished) Varun Pant Blog | Index In the last line, one can see that the page title <strong>Varun Pant Blog | Index</strong> was printed.</p><p>The principle of a web scraper is generally quite simple,</p><ol start=2><li>Start from a seed url and extract all hyperlinks.</li><li>Then crawl each of them, further extracting links until all of them have been visited.
In Scrapy start_urls is the list of seed urls to begin scraping.</li></ol><p>One can also provide another varibale to add domain constraint. allowed_domains = [&ldquo;varunpant.com&rdquo;] so that links which lead out of the principal domain are rejected.</p><p>Scrappy takes care of ensuring that links which have already been visited are not visited again.</p><h3 id=extracting-links>Extracting Links</h3><p>In the simple example above, the code does not extract hyperlinks from the response body, lets modify it to do so.</p><p>import scrapy class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; start_urls = [&lsquo;<a href="https://varunpant.com'">https://varunpant.com'</a>] def parse(self, response): all_links = response.xpath('<em>//a/@href&rsquo;).extract() for link in all_links: print(link) Line all_links = response.xpath('</em>//a/@href&rsquo;).extract() , uses xpath and extracts all hyperlinks in the page.</p><p>/ /contact /archives /feed /posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner <a href=http://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner#disqus>http://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner#disqus</a>_thread /posts/gdal-2-on-mac-with-homebrew <a href=http://varunpant.com/posts/gdal-2-on-mac-with-homebrew#disqus>http://varunpant.com/posts/gdal-2-on-mac-with-homebrew#disqus</a>_thread /posts/how-to-print-bar-chart-in-chrome-browser-console <a href=http://varunpant.com/posts/how-to-print-bar-chart-in-chrome-browser-console#disqus>http://varunpant.com/posts/how-to-print-bar-chart-in-chrome-browser-console#disqus</a>_thread /posts/how-to-make-https-requests-with-python-httplib2-ssl <a href=http://varunpant.com/posts/how-to-make-https-requests-with-python-httplib2-ssl#disqus>http://varunpant.com/posts/how-to-make-https-requests-with-python-httplib2-ssl#disqus</a>_thread /posts/how-to-read-json-from-web-http-request-of-url-via-python-urllib <a href=http://varunpant.com/posts/how-to-read-json-from-web-http-request-of-url-via-python-urllib#disqus>http://varunpant.com/posts/how-to-read-json-from-web-http-request-of-url-via-python-urllib#disqus</a>_thread /posts/minimum-insertions-to-form-a-palindrome <a href=http://varunpant.com/posts/minimum-insertions-to-form-a-palindrome#disqus>http://varunpant.com/posts/minimum-insertions-to-form-a-palindrome#disqus</a>_thread /posts/inverse-weighted-distance-interpolation-in-golang <a href=http://varunpant.com/posts/inverse-weighted-distance-interpolation-in-golang#disqus>http://varunpant.com/posts/inverse-weighted-distance-interpolation-in-golang#disqus</a>_thread /posts/basic-sorting-algorithms-implemented-in-golang <a href=http://varunpant.com/posts/basic-sorting-algorithms-implemented-in-golang#disqus>http://varunpant.com/posts/basic-sorting-algorithms-implemented-in-golang#disqus</a>_thread /posts/reading-and-writing-binary-files-in-go-lang <a href=http://varunpant.com/posts/reading-and-writing-binary-files-in-go-lang#disqus>http://varunpant.com/posts/reading-and-writing-binary-files-in-go-lang#disqus</a>_thread /posts/create-linear-color-gradient-in-go <a href=http://varunpant.com/posts/create-linear-color-gradient-in-go#disqus>http://varunpant.com/posts/create-linear-color-gradient-in-go#disqus</a>_thread /?page=2 <a href=http://stackoverflow.com/users/95967>http://stackoverflow.com/users/95967</a> <a href=https://www.linkedin.com/in/varunpant>https://www.linkedin.com/in/varunpant</a> mailto:varun@varunpant.com <a href=https://github.com/varunpant>https://github.com/varunpant</a> The final step is to make the parse generator method yield response like so</p><p>import scrapy class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; start_urls = [&lsquo;<a href="https://varunpant.com'">https://varunpant.com'</a>] allowed_domains = [&ldquo;varunpant.com&rdquo;] def parse(self, response): page_title = response.xpath('//title/text()').extract_first() print(page_title) all_links = response.xpath('*//a/@href&rsquo;).extract() for link in all_links: yield scrapy.Request( response.urljoin(link), callback=self.parse ) This should crawl all hyperlinks found with each page.</p><h3 id=save-all-crawlled-pages-to-link>Save all crawlled pages to link</h3><p>Here is a minimalistic code snippet, which saves all crawlled pages to the disk.</p><p>import scrapy import unicodedata import re from pprint import pprint class MySpider(scrapy.Spider): name = &ldquo;MySpider&rdquo; allowed_domains = [&ldquo;varunpant.com&rdquo;] start_urls = [&lsquo;<a href="https://varunpant.com'">https://varunpant.com'</a>] def slugify(self,value): value = unicodedata.normalize(&lsquo;NFKD&rsquo;, value).encode(&lsquo;ascii&rsquo;, &lsquo;ignore&rsquo;) value = unicode(re.sub('[^\w\s-]', &lsquo;', value).strip().lower()) value = unicode(re.sub('[-\s]+&rsquo;, &lsquo;-', value)) return value def save(self,name,content): #save to html in the pages folder p = &ldquo;pages/%s.html&rdquo; fn = self.slugify(name) with open(p%name,&ldquo;w&rdquo;) as f: f.write(content.encode(&ldquo;utf-8&rdquo;)) def parse(self, response): page_title = response.xpath('//title/text()').extract_first() page_body = response.body.decode(&ldquo;utf-8&rdquo;) self.save(page_title,page_body) all_links = response.xpath('*//a/@href&rsquo;).extract() for link in all_links: yield scrapy.Request( response.urljoin(link), callback=self.parse )</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=/posts/download-file-using-webdriver-firefox-and-python-selenium/><span class=button__icon>←</span>
<span class=button__text>download file using webdriver firefox and python selenium</span></a></span>
<span class="button next"><a href=/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/><span class=button__text>integration testing with apache beam using pubsub and bigtable emulators and direct runner</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2019 Powered by <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by <a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=/assets/main.js></script><script src=/assets/prism.js></script></div></body></html>
<!doctype html><html lang=en><head><title>integration testing with apache beam using pubsub and bigtable emulators and direct runner :: Varun Pant — WebLog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &amp;ldquo;end-to-end&amp;rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta name=keywords content="Cloud,maps,GIS,Google Cloud,Google Maps,Openlayer,Javascript,go,Hadoop,BigData,Spark"><meta name=robots content="noodp"><link rel=canonical href=https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/><link type=application/opensearchdescription+xml rel=search title="Varun Pant" href=//www.varunpant.com/opensearch.axd><link rel=me type=text/html href="//plus.google.com/108455320594011020517?rel=me"><link rel=me type=text/html href=//varunpant.com/feed><link rel=me type=text/html href=//twitter.com/varunpant><link rel=me type=text/html href=//www.facebook.com/varun.pant><meta name=msvalidate.01 content="B96ED99B0213B547253C91D27272A5E4"><meta name=google-site-verification content="yu9tlev9-FkUUc0UkPrKP-8ren886--1FcSINU0TJDY"><meta http-equiv=x-xrds-location content="//www.myopenid.com/xrds?username=varunpant.myopenid.com"><meta name=syndication content="//feeds.feedburner.com/varun.pant"><link rel=stylesheet href=https://varunpant.com/assets/style.css><link rel=stylesheet href=https://varunpant.com/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://varunpant.com/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://varunpant.com/img/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:title content="integration testing with apache beam using pubsub and bigtable emulators and direct runner"><meta name=twitter:description content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta property="og:title" content="integration testing with apache beam using pubsub and bigtable emulators and direct runner"><meta property="og:description" content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta property="og:type" content="article"><meta property="og:url" content="https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/"><meta property="article:published_time" content="2018-08-03T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-03T00:00:00+00:00"><meta property="og:site_name" content="Varun Pant"><section><div id=ta class=advert><script type=text/javascript>google_ad_client="ca-pub-1110730152886910";google_ad_slot="7384719372";google_ad_width=468;google_ad_height=60;</script><script type=text/javascript src=//pagead2.googlesyndication.com/pagead/show_ads.js></script></div></section></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/archives>Archives</a></li><li><a href=/feed.xml>Feed</a></li><li><a href=/showcase>Showcase</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h2 class=post-title><a href=https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/>integration testing with apache beam using pubsub and bigtable emulators and direct runner</a></h2><div class=post-meta><span class=post-date>2018-08-03</span>
<span class=post-author>— Varun Pant</span>
<span class=post-read-time>— 4 min read</span></div><span class=post-tags>#<a href=https://varunpant.com/tags/dataflow/>dataflow</a>&nbsp;
#<a href=https://varunpant.com/tags/apache-beam/>apache-beam</a>&nbsp;
#<a href=https://varunpant.com/tags/java/>java</a>&nbsp;
#<a href=https://varunpant.com/tags/google/>google</a>&nbsp;
#<a href=https://varunpant.com/tags/bigdata/>bigdata</a>&nbsp;
#<a href=https://varunpant.com/tags/testing/>testing</a>&nbsp;</span><div class=post-content><h3 id=summary>Summary</h3><p>Recently I have been looking into ways to test my <strong>Apache Beam</strong> pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.</p><p>A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky.</p><p>Apache beam has various <a href=https://beam.apache.org/documentation/runners/capability-matrix/>runners</a>, the&rdquo; one of interest&rdquo; for testing purposes is the <a href=https://beam.apache.org/documentation/runners/direct/>Direct runner</a>.</p><p>From the doc:</p><blockquote><p>The Direct Runner executes pipelines on your machine and is designed to validate that pipelines adhere to the Apache Beam model as closely as possible. Instead of focusing on efficient pipeline execution, the Direct Runner performs additional checks to ensure that users do not rely on semantics that are not guaranteed by the model. Some of these checks include:</p><ul><li>enforcing immutability of elements.</li><li>enforcing encodability of elements.</li><li>elements are processed in an arbitrary order at all points.</li><li>serialization of user functions (DoFn, CombineFn, etc.)</li></ul><p>It’s also important to realise the memory considerations as mentioned below.</p></blockquote><blockquote><p>Local execution is limited by the memory available in your local environment. It is highly recommended that you run your pipeline with data sets small enough to fit in local memory. You can create a small in-memory data set using a Create transform, or you can use a Read transform to work with small local or remote files. I am going to show an example where I have written a basic integration test which listens for some payload from pubsub emulator, transforms the data to Mutation and then writes it to BigTable emulator, there are no aggregations performed hence every thing works in global window and triggering of a write is immediate. This example can also serve as a good way to front your data warehouse with apache beam for dynamically scalable writing,i.e as the pubsub message load would increase, beam would add more workers and as the load would decrease beam would reduce the workers.</p></blockquote><h4 id=setup-pubsub-emulator>SetUp PubSub Emulator</h4><p>Guide is <a href=https://cloud.google.com/pubsub/docs/emulator>here</a>.</p><h5 id=code>Code</h5><pre><code class=language-gcloud data-lang=gcloud>gcloud components update
gcloud beta emulators pubsub start [options]

</code></pre><p>optional step</p><pre><code class=language-$(gcloud data-lang=$(gcloud>
</code></pre><p>*You don’t need to run the optional step above as we would supply the url in the beam options. *</p><h4 id=setup-bigtable-emulator>SetUp BigTable Emulator</h4><p>Guide is <a href=https://cloud.google.com/bigtable/docs/emulator>here</a>.</p><h5 id=code-1>Code</h5><pre><code class=language-gcloud data-lang=gcloud>gcloud beta emulators bigtable start

</code></pre><p>*This step is required if you want to use <a href=https://cloud.google.com/bigtable/docs/cbt-overview>cbt</a> to point to BigTable to browse the data. *</p><pre><code class=language-$(gcloud data-lang=$(gcloud>
</code></pre><h3 id=code-2>Code</h3><p>Here are the steps involved in the pipeline.</p><pre><code class=language-Pipeline data-lang=Pipeline>
       p.apply(&quot;ReadPubsubMessages&quot;,
              
              PubsubIO.readMessages().fromSubscription(options.getSubscription()))

              .apply(&quot;ConvertMessageToBTMutation&quot;, new PubsubMessageToBigTableMutation())


               .apply(&quot;WriteToBigTable&quot;,

                       CloudBigtableIO.writeToTable(

                               getConfigurationForTable(options).withTableId(BIGTABLE\_TABLE\_ID).build()
               ));

       p.run().waitUntilFinish();

</code></pre><p>The pipeline is triggered via main method which is extended to include a testing variable</p><pre><code class=language-public data-lang=public> 
       @Description(&quot;Pub/Sub subscription to read from&quot;)
       @Validation.Required
       @Default.String(&quot;projects/test-project/subscriptions/evtsToBigTbl&quot;)
       String getSubscription();

       void setSubscription(String value);

       @Description(&quot;The Google Cloud project ID for the Cloud Bigtable instance.&quot;)
       @Validation.Required
       @Default.String(&quot;BT-PROD-PROJECT&quot;)
       String getBigtableProjectId();

       void setBigtableProjectId(String bigtableProjectId);

       @Description(&quot;The Google Cloud Bigtable instance ID .&quot;)
       @Validation.Required
       @Default.String(&quot;BT-PROD-INSTANCE&quot;)
       String getBigtableInstanceId();

       void setBigtableInstanceId(String bigtableInstanceId);


       @Description(&quot;For integration test.&quot;)
       @Validation.Required
       @Default.Boolean(true)
       Boolean getTesting();

       void setTesting(Boolean testing);


   }

</code></pre><p>Insure that the project and instance id for bigtable matches those in the cofiguration file ~/.cbtrc for bigtable, you can check this by typing in cbt in the console.</p><pre><code class=language-EventListenerOptions data-lang=EventListenerOptions>               .as(EventListenerOptions.class);
       options.setProject(&quot;PubSubToBigTable&quot;);

       if (options.getTesting()) {

           options.setPubsubRootUrl(PUBSUB\_EMULATOR\_HOST);//must point to emulator http://localhost:8085
           options.setBigtableProjectId(BIGTABLE\_PROJECT\_ID);//must be the same as in ~/.cbtrc
           options.setBigtableInstanceId(BIGTABLE\_INSTANCE\_ID);//must be the same as in ~/.cbtrc

       }

       RunPipeLine(options);


</code></pre><p>The intreseting bits are as follows</p><h3 id=override-pubsuboptions-url-in-beam>Override PubsubOptions url in Beam</h3><p>This was easy, just make sure your options extend org.apache.beam.sdk.io.gcp.pubsub.PubsubOptions. This includes a method called options.setPubsubRootUrl, whcih then can be pointed to emulator.</p><h3 id=point-beam-to-bigtable-emulator>Point beam to BigTable Emulator.</h3><p>This took me ages to figure out and eventually after digging in the code I was able to locate a bunch of properties which must be overridden.</p><pre><code class=language-private data-lang=private>
      CloudBigtableTableConfiguration.Builder config = new CloudBigtableTableConfiguration.Builder()
               .withProjectId(options.getBigtableProjectId())
               .withInstanceId(options.getBigtableInstanceId());
               
       if (options.getTesting()) {
           config.withConfiguration(&quot;google.bigtable.instance.admin.endpoint.host&quot;, &quot;localhost&quot;)
                   .withConfiguration(&quot;google.bigtable.admin.endpoint.host&quot;, &quot;localhost&quot;)
                   .withConfiguration(&quot;google.bigtable.endpoint.host&quot;, &quot;localhost&quot;)
                   .withConfiguration(&quot;google.bigtable.endpoint.port&quot;, &quot;8086&quot;)
                   .withConfiguration(&quot;google.bigtable.use.plaintext.negotiation&quot;, &quot;true&quot;)
                   .withConfiguration(&quot;google.bigtable.grpc.read.partial.row.timeout.ms&quot;, &quot;5000&quot;);
       }

       return config;


   }

</code></pre><p>and thats it, calling StarterPipeline.main(new String[]{"&ndash;testing=true&rdquo;, &ldquo;&ndash;runner=DirectRunner&rdquo;}); should do the trick.</p><p>I will soon include full code.</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://varunpant.com/posts/web-crawling-or-scraping-using-scrapy-in-python/><span class=button__icon>←</span>
<span class=button__text>web crawling or scraping using scrapy in python</span></a></span>
<span class="button next"><a href=https://varunpant.com/posts/gdal-2-on-mac-with-homebrew/><span class=button__text>gdal 2 on mac with homebrew</span>
<span class=button__icon>→</span></a></span></div></div><div id=blog-comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"varunpant"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2020 Powered by <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by <a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://varunpant.com/assets/main.js></script><script src=https://varunpant.com/assets/prism.js></script><section><div class=advert id=ta2><script type=text/javascript>google_ad_client="ca-pub-1110730152886910";google_ad_slot="1639236970";google_ad_width=728;google_ad_height=90;</script><script type=text/javascript src=//pagead2.googlesyndication.com/pagead/show_ads.js></script></div></section><script id=dsq-count-scr src=//varunpant.disqus.com/count.js async></script></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-16978408-1','auto');ga('send','pageview');}</script></body></html>
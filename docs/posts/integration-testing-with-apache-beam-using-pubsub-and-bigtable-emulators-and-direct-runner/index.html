<!doctype html><html lang=en><head><title>integration testing with apache beam using pubsub and bigtable emulators and direct runner :: Varun Pant — WebLog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &amp;ldquo;end-to-end&amp;rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta name=keywords content="Cloud,maps,GIS,Google Cloud,Google Maps,Openlayer,Javascript,go,Hadoop,BigData,Spark"><meta name=robots content="noodp"><link rel=canonical href=https://varunpant.github.io/weblog/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/><link rel=stylesheet href=https://varunpant.github.io/weblog/assets/style.css><link rel=stylesheet href=https://varunpant.github.io/weblog/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=https://varunpant.github.io/weblog/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=https://varunpant.github.io/weblog/img/favicon.png><meta name=twitter:card content="summary"><meta name=twitter:title content="integration testing with apache beam using pubsub and bigtable emulators and direct runner"><meta name=twitter:description content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta property="og:title" content="integration testing with apache beam using pubsub and bigtable emulators and direct runner"><meta property="og:description" content="Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky."><meta property="og:type" content="article"><meta property="og:url" content="https://varunpant.github.io/weblog/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/"><meta property="article:published_time" content="2018-08-03T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-03T00:00:00+00:00"><meta property="og:site_name" content="Varun Pant"></head><body><div class=container><header class=header><span class=header__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><span class=header__right><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/weblog/about>About</a></li><li><a href=/weblog/archives>Archives</a></li><li><a href=/weblog/feed.xml>Feed</a></li><li><a href=/weblog/showcase>Showcase</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/weblog/about>About</a></li><li><a href=/weblog/archives>Archives</a></li><li><a href=/weblog/feed.xml>Feed</a></li><li><a href=/weblog/showcase>Showcase</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class=theme-toggle><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41C32.4934 41 41 32.4934 41 22 41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><div class=post><h2 class=post-title><a href=https://varunpant.github.io/weblog/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/>integration testing with apache beam using pubsub and bigtable emulators and direct runner</a></h2><div class=post-meta><span class=post-date>2018-08-03</span>
<span class=post-author>— Varun Pant</span>
<span class=post-read-time>— 4 min read</span></div><div class=post-content><h3 id=summary>Summary</h3><p>Recently I have been looking into ways to test my <strong>Apache Beam</strong> pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.</p><p>A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &ldquo;end-to-end&rdquo; testing or “Integration testing” and this is where things can sometimes get tricky.</p><p>Apache beam has various <a href=https://beam.apache.org/documentation/runners/capability-matrix/>runners</a>, the&rdquo; one of interest&rdquo; for testing purposes is the <a href=https://beam.apache.org/documentation/runners/direct/>Direct runner</a>.</p><p>From the doc:</p><blockquote><p>The Direct Runner executes pipelines on your machine and is designed to validate that pipelines adhere to the Apache Beam model as closely as possible. Instead of focusing on efficient pipeline execution, the Direct Runner performs additional checks to ensure that users do not rely on semantics that are not guaranteed by the model. Some of these checks include: * enforcing immutability of elements.</p><ul><li>enforcing encodability of elements.</li><li>elements are processed in an arbitrary order at all points.</li><li>serialization of user functions (DoFn, CombineFn, etc.)
It’s also important to realise the memory considerations as mentioned below.</li></ul></blockquote><blockquote><p>Local execution is limited by the memory available in your local environment. It is highly recommended that you run your pipeline with data sets small enough to fit in local memory. You can create a small in-memory data set using a Create transform, or you can use a Read transform to work with small local or remote files. I am going to show an example where I have written a basic integration test which listens for some payload from pubsub emulator, transforms the data to Mutation and then writes it to BigTable emulator, there are no aggregations performed hence every thing works in global window and triggering of a write is immediate. This example can also serve as a good way to front your data warehouse with apache beam for dynamically scalable writing,i.e as the pubsub message load would increase, beam would add more workers and as the load would decrease beam would reduce the workers.</p></blockquote><h4 id=setup-pubsub-emulator>SetUp PubSub Emulator</h4><p>Guide is <a href=https://cloud.google.com/pubsub/docs/emulator>here</a>.</p><h5 id=code>Code</h5><p>gcloud components install pubsub-emulator gcloud components update gcloud beta emulators pubsub start [options] optional step</p><p>$(gcloud beta emulators pubsub env-init) *You don’t need to run the optional step above as we would supply the url in the beam options. *</p><h4 id=setup-bigtable-emulator>SetUp BigTable Emulator</h4><p>Guide is <a href=https://cloud.google.com/bigtable/docs/emulator>here</a>.</p><h5 id=code-1>Code</h5><p>gcloud components update beta gcloud beta emulators bigtable start *This step is required if you want to use <a href=https://cloud.google.com/bigtable/docs/cbt-overview>cbt</a> to point to BigTable to browse the data. *</p><p>$(gcloud beta emulators bigtable env-init) ### Code</p><p>Here are the steps involved in the pipeline.</p><p>Pipeline p = Pipeline.create(options); p.apply(&ldquo;ReadPubsubMessages&rdquo;, PubsubIO.readMessages().fromSubscription(options.getSubscription())) .apply(&ldquo;ConvertMessageToBTMutation&rdquo;, new PubsubMessageToBigTableMutation()) .apply(&ldquo;WriteToBigTable&rdquo;, CloudBigtableIO.writeToTable( getConfigurationForTable(options).withTableId(BIGTABLE_TABLE_ID).build() )); p.run().waitUntilFinish(); The pipeline is triggered via main method which is extended to include a testing variable</p><p>public interface EventListenerOptions extends StreamingOptions, PubsubOptions { @Description(&ldquo;Pub/Sub subscription to read from&rdquo;) @Validation.Required @Default.String(&ldquo;projects/test-project/subscriptions/evtsToBigTbl&rdquo;) String getSubscription(); void setSubscription(String value); @Description(&ldquo;The Google Cloud project ID for the Cloud Bigtable instance.") @Validation.Required @Default.String(&ldquo;BT-PROD-PROJECT&rdquo;) String getBigtableProjectId(); void setBigtableProjectId(String bigtableProjectId); @Description(&ldquo;The Google Cloud Bigtable instance ID .") @Validation.Required @Default.String(&ldquo;BT-PROD-INSTANCE&rdquo;) String getBigtableInstanceId(); void setBigtableInstanceId(String bigtableInstanceId); @Description(&ldquo;For integration test.") @Validation.Required @Default.Boolean(true) Boolean getTesting(); void setTesting(Boolean testing); } Insure that the project and instance id for bigtable matches those in the cofiguration file ~/.cbtrc for bigtable, you can check this by typing in cbt in the console.</p><p>EventListenerOptions options = PipelineOptionsFactory.fromArgs(args).withValidation() .as(EventListenerOptions.class); options.setProject(&ldquo;PubSubToBigTable&rdquo;); if (options.getTesting()) { options.setPubsubRootUrl(PUBSUB_EMULATOR_HOST);//must point to emulator http://localhost:8085 options.setBigtableProjectId(BIGTABLE_PROJECT_ID);//must be the same as in ~/.cbtrc options.setBigtableInstanceId(BIGTABLE_INSTANCE_ID);//must be the same as in ~/.cbtrc } RunPipeLine(options); The intreseting bits are as follows</p><h3 id=override-pubsuboptions-url-in-beam>Override PubsubOptions url in Beam</h3><p>This was easy, just make sure your options extend org.apache.beam.sdk.io.gcp.pubsub.PubsubOptions. This includes a method called options.setPubsubRootUrl, whcih then can be pointed to emulator.</p><h3 id=point-beam-to-bigtable-emulator>Point beam to BigTable Emulator.</h3><p>This took me ages to figure out and eventually after digging in the code I was able to locate a bunch of properties which must be overridden.</p><p>private static CloudBigtableTableConfiguration.Builder getConfigurationForTable(EventListenerOptions options) { CloudBigtableTableConfiguration.Builder config = new CloudBigtableTableConfiguration.Builder() .withProjectId(options.getBigtableProjectId()) .withInstanceId(options.getBigtableInstanceId()); if (options.getTesting()) { config.withConfiguration(&ldquo;google.bigtable.instance.admin.endpoint.host&rdquo;, &ldquo;localhost&rdquo;) .withConfiguration(&ldquo;google.bigtable.admin.endpoint.host&rdquo;, &ldquo;localhost&rdquo;) .withConfiguration(&ldquo;google.bigtable.endpoint.host&rdquo;, &ldquo;localhost&rdquo;) .withConfiguration(&ldquo;google.bigtable.endpoint.port&rdquo;, &ldquo;8086&rdquo;) .withConfiguration(&ldquo;google.bigtable.use.plaintext.negotiation&rdquo;, &ldquo;true&rdquo;) .withConfiguration(&ldquo;google.bigtable.grpc.read.partial.row.timeout.ms&rdquo;, &ldquo;5000&rdquo;); } return config; } and thats it, calling StarterPipeline.main(new String[]{"&ndash;testing=true&rdquo;, &ldquo;&ndash;runner=DirectRunner&rdquo;}); should do the trick.</p><p>I will soon include full code.</p></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://varunpant.github.io/weblog/posts/web-crawling-or-scraping-using-scrapy-in-python/><span class=button__icon>←</span>
<span class=button__text>web crawling or scraping using scrapy in python</span></a></span>
<span class="button next"><a href=https://varunpant.github.io/weblog/posts/gdal-2-on-mac-with-homebrew/><span class=button__text>gdal 2 on mac with homebrew</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><a href=/ class=logo style=text-decoration:none><span class=logo__mark><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44"><path fill="none" d="M15 8l14.729 14.382L15 35.367"/></svg></span><span class=logo__text>Varun Pant</span>
<span class=logo__cursor></span></a><div class=copyright><span>© 2019 Powered by <a href=https://gohugo.io target=_blank rel=noopener>Hugo</a></span>
<span>Theme created by <a href=https://twitter.com/panr target=_blank rel=noopener>panr</a></span></div></div></footer><script src=https://varunpant.github.io/weblog/assets/main.js></script><script src=https://varunpant.github.io/weblog/assets/prism.js></script></div></body></html>
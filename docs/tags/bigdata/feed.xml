<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bigdata on Varun Pant</title><link>https://varunpant.com/tags/bigdata/</link><description>Recent content in bigdata on Varun Pant</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 03 Aug 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://varunpant.com/tags/bigdata/feed.xml" rel="self" type="application/rss+xml"/><item><title>integration testing with apache beam using pubsub and bigtable emulators and direct runner</title><link>https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/</link><pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate><guid>https://varunpant.com/posts/integration-testing-with-apache-beam-using-pubsub-and-bigtable-emulators-and-direct-runner/</guid><description>Summary Recently I have been looking into ways to test my Apache Beam pipelines at work. Most common use cases of Beam generally involves either batch reading data from GCS and writing to analytical platforms such as Big Query or stream reading data from Pubsub and writing to perhaps Bigtable.
A pipelines consists of transforms and its generally easy to test them in isolation as a independent unit test per stage, however I am personally a big fan of &amp;ldquo;end-to-end&amp;rdquo; testing or “Integration testing” and this is where things can sometimes get tricky.</description></item></channel></rss>